## Standardize Resolution and Frame Rate

```mkdir -p processed_clips
for file in clip_*.mp4; do
    ffmpeg -i "$file" -vf scale=854:480 -r 24 -c:v libx264 -crf 18 -c:a aac -q:v 2 "processed_clips/${file%.*}_processed.mp4"
done```



steps for lora training 

1. make sure wan model is downloaded
2. Transfer your dataset/10_dragonball/ folder to the server (e.g., using SCP): or do the clipping again on the server

## command for lora training

```bash
```accelerate launch --mixed_precision="fp16" train_lora.py```


### Wan 2.1 T2V-1.3B: this is the model to finetune 


###############################################################################
1. **Prepare Server Environment**: Ensure the server has a compatible OS (e.g., Linux/Ubuntu), Python 3.9+, CUDA 11.x+, and sufficient GPU memory (e.g., 16GB+ for batch size 2). Install dependencies (`torch`, `diffusers`, `peft`, `torchvision`, `opencv-python`) using `pip install -r requirements.txt` with a `requirements.txt` listing these.

2. **Upload Code and Data**: Transfer `train_lora.py`, `fm_solvers.py`, and the Dragon Ball dataset (including `metadata.json`) to the server via SCP or SFTP (e.g., `scp -r ./project user@server:/path/`).

3. **Set Up Virtual Environment**: Create a virtual environment (`python -m venv venv`) and activate it (`source venv/bin/activate`) to isolate dependencies.

4. **Install Model Weights**: Download or copy the Wan 2.1 model (`Wan2.1-T2V-1.3B`) to the server (e.g., via `git lfs` or manual upload) and adjust the `base_model` path in `train_lora.py` if needed.

5. **Configure GPU Access**: Verify CUDA is detected (`nvidia-smi`) and set `CUDA_VISIBLE_DEVICES` (e.g., `export CUDA_VISIBLE_DEVICES=0`) to use the desired GPU.

6. **Test Locally**: Run a small test on the server (e.g., `python train_lora.py` with a single batch) to check for errors in imports, paths, or GPU compatibility.

7. **Optimize for Server**: Adjust `batch_size` (e.g., to 1) and `num_epochs` based on server VRAM, and use `accelerate` (`accelerate config` then `accelerate launch --mixed_precision="fp16" train_lora.py`) for multi-GPU or memory efficiency.

8. **Monitor Execution**: Run the script in a screen or tmux session (`screen` then `python train_lora.py`) and monitor logs (e.g., loss values) to ensure training progresses.

9. **Handle Output**: Ensure the `output/dragonball_lora` directory is writable, and after training, copy the `.safetensors` file back to your local machine (`scp user@server:/path/output/dragonball_lora/* .`) for validation.

10. **Debug Issues**: If errors occur (e.g., OOM, import failures), check GPU usage, adjust batch size, or consult logs/server error messages, and adjust paths or dependencies accordingly.




############################################################3
Alternative: Video-LLaVA for Full Clip Analysis
we will currently  use blip-2 for the training, but if it fails we will use video-llava. blip-2 is more lightweight because the caption is generated by sampling the vidos, while video-llava requires the full video to be processed.
manual check is absolutely necesssry 